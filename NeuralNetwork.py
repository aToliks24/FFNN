import numpy as np

np.random.seed(555)


def initialize_parameters(layer_dims):
    """"
    input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output sigmoid)
    output: a dictionary containing the initialized W and b parameters of each layer (W1â€¦WL, b1â€¦bL).
    """""
    init_params = {}
    for i, layer in enumerate(layer_dims[1:]):
        init_params['W'+str(i)] = np.random.rand(layer_dims[i],layer_dims[i-1]) # should we use specific range for this initialization?
        init_params['b'+str(i)] = np.zeros(shape=(layer_dims[i],1))

    return init_params

def linear_forward(A, W, b):
    """
    Description:
    Implement the linear part of a layer's forward propagation.
    input:
    A â€“ the activations of the previous layer
    W â€“ the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    B â€“ the bias vector of the current layer (of shape [size of current layer, 1])
    Output:
    Z â€“ the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache â€“ a dictionary containing A, W, b and Z (stored for making the backpropagation easier to compute)
    """
    Z = np.dot(W,A) + b # broadcasting
    linear_cach = {"A": A, "W": W, "b": b, "Z": Z}

    return Z, linear_cach


def sigmoid(Z):
    """
    Input:
    Z â€“ the linear component of the activation function
    Output:
    A â€“ the activations of the layer
    activation_cache â€“ returns Z, which will be useful for the backpropagation
    """
    A = 1.0 / (1 + np.exp(-1.0*Z))
    activation_cache = {"Z": Z}

    return A, activation_cache


def relu(Z):
    """
    Input:
    Z â€“ the linear component of the activation function
    Output:
    A â€“ the activations of the layer
    activation_cache â€“ returns Z, which will be useful for the backpropagation
    """
    A = np.maximum(0,Z)
    activation_cache = {"Z": Z}

    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation):
    """
    Description:
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    Input:
    A_prev â€“ activations of the previous layer
    W â€“ the weights matrix of the current layer
    B â€“ the bias vector of the current layer
    Activation â€“ the activation function to be used (a string, either â€œsigmoidâ€ or â€œreluâ€)
    Output:
    A â€“ the activations of the current layer
    linear_cache â€“ the dictionary generated by the linear_forward function
    """

    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation == "sigmoid":
        return sigmoid(Z)
    elif activation == "relu":
        return relu(Z)


def L_model_forward(X, parameters):
    """
    Description: Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    Input:
    X â€“ the data, numpy array of shape (input size, number of examples) parameters â€“ the initialized W and b parameters of each layer
    Output:
    AL â€“ the last post-activation value
    caches â€“ a list of all the cache objects generated by the linear_forward function
    """
    caches = []
    L = len(parameters) / 2 # parameters is a dictionary which holds Wl and bl for each layer l
    A = X
    for i in xrange(1,L):
        A, linear_cache = linear_activation_forward(A,parameters['W'+str(i)], parameters['b'+str(i)], "relu")
        caches.append(linear_cache)
    AL, linear_cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], "sigmoid")
    caches.append(linear_cache)

    return AL, caches

def compute_cost(AL, Y):
    """
    Description: Implement the cost function defined by equation
    ğ‘ğ‘œğ‘ ğ‘¡=âˆ’1ğ‘šâˆ—Î£[(ğ‘¦ğ‘–âˆ—log(ğ´ğ¿))+((1âˆ’ğ‘¦ğ‘–)âˆ—(1âˆ’ğ´ğ¿))]ğ‘š1 (see the slides of the first lecture for additional information if needed).
    Input:
    AL â€“ probability vector corresponding to your label predictions, shape (1, number of examples)
    Y â€“ the labels vector (i.e. the ground truth)
    Output:
    cost â€“ the cross-entropy cost
    """
    #TODO verify len(Y): Y is (m,1) or(1,m)??
    cost = -1.0/len(Y) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))
    return cost

# Old code
# class ActivationFunction(object):
#     def activate(self,X,W,b):
#         pass
#     def inv_activate(self,X,W,b):
#         pass
#
#
#
# class LinearActivation(ActivationFunction):
#     def activate(self,X,W,b):
#         return np.transpose(W)*X+b
#
#     def inv_activate(self,X,W,b):
#         return (X-b)/np.transpose(W)
#
#
# class Perceptron(object):
#     _activations = {'linear': LinearActivation}
#
#     def __init__(self,n_inputs,weights=None,activation='linear',seed=None):
#         np.random.seed(seed)
#         self._activation=self._activations[activation]
#         self._inputs=n_inputs
#         self._weights=np.random.rand(n_inputs,1)
#         if weights!=None:
#             self._weights=weights
#
#     def update_weights(self, new_weights):
#         self._weights=new_weights





