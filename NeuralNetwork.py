# coding=utf-8
import numpy as np
import idx2numpy

np.random.seed(555)

def load_data_set(numbers_classes):
    """
    
    :param numbers_classes: list of size 2, with the numbers we want to select from the dataset 
            usage example load_data_set([3,8]) where the label for 3 is 0 and the label for 8 is 1
    :return: (x_train, y_train), (x_test, y_test) --- X â€“ the data, numpy array of shape (input size (features), number of examples)
    """

    # train

    x_train = idx2numpy.convert_from_file('data/train-images.idx3-ubyte')  # shape ((m_train, 28, 28)
    x_train = x_train.reshape(x_train.shape[0], -1)  # shape (m_train,784)
    print("    x_train = x_train.reshape(x_train.shape[0], -1).T:   ", x_train.shape)

    y_train = idx2numpy.convert_from_file('data/train-labels.idx1-ubyte')
    #y_train = np.expand_dims(y_train, axis=0)
    print("np.expand_dims(y_train, axis=0):    ", y_train.shape)

    mask = np.vectorize(lambda t: True if t in numbers_classes else False)
    train_mask = mask(y_train)
    print("train_mask = mask(y_train):     ", train_mask.shape)
    x_train = x_train[train_mask]
    y_train = y_train[train_mask]

    class_to_binary = np.vectorize(lambda t: 0 if t == numbers_classes[0] else 1)
    y_train = class_to_binary(y_train)

    # test

    x_test = idx2numpy.convert_from_file('data/t10k-images.idx3-ubyte')  # shape ((m_test, 28, 28)
    x_test = x_test.reshape(x_test.shape[0], -1)  # shape (m_test, 784 )

    y_test = idx2numpy.convert_from_file('data/t10k-labels.idx1-ubyte')
    #y_test = np.expand_dims(y_test, axis=0)

    test_mask = mask(y_test)

    x_test = x_test[test_mask]
    y_test = y_test[test_mask]

    y_test = class_to_binary(y_test)

    # shapes: ((784, m_train), (1, m_train)) ((784, m_test), (1, m_test))
    return (x_train.T, np.expand_dims(y_train, axis=0)), (x_test.T, np.expand_dims(y_test, axis=0))




def initialize_parameters(layer_dims):
    """
    input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output sigmoid)
    output: a dictionary containing the initialized W and b parameters of each layer (W1â€¦WL, b1â€¦bL).
    """
    init_params = {}
    for i, layer in enumerate(layer_dims[1:]):
        print(i)
        init_params['W'+str(i+1)] = np.random.rand(layer_dims[i+1],layer_dims[i]) # should we use specific range for this initialization?
        init_params['b'+str(i+1)] = np.zeros(shape=(layer_dims[i+1],1))

    return init_params

def linear_forward(A, W, b):
    """
    Description:
    Implement the linear part of a layer's forward propagation.
    input:
    A â€“ the activations of the previous layer
    W â€“ the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    B â€“ the bias vector of the current layer (of shape [size of current layer, 1])
    Output:
    Z â€“ the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache â€“ a dictionary containing A, W, b and Z (stored for making the backpropagation easier to compute)
    """
    Z = np.dot(W,A) + b # broadcasting
    linear_cache = {"A": A, "W": W, "b": b, "Z": Z}  # TODO: Remove Z from cach dictionary

    return Z, linear_cache


def sigmoid(Z):
    """
    Input:
    Z â€“ the linear component of the activation function
    Output:
    A â€“ the activations of the layer
    activation_cache â€“ returns Z, which will be useful for the backpropagation
    """
    A = 1.0 / (1 + np.exp(-1.0*Z))
    activation_cache = {"Z": Z}

    return A, activation_cache


def relu(Z):
    """
    Input:
    Z â€“ the linear component of the activation function
    Output:
    A â€“ the activations of the layer
    activation_cache â€“ returns Z, which will be useful for the backpropagation
    """
    A = np.maximum(0,Z)
    activation_cache = {"Z": Z}

    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation):
    """
    Description:
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    Input:
    A_prev â€“ activations of the previous layer
    W â€“ the weights matrix of the current layer
    B â€“ the bias vector of the current layer
    Activation â€“ the activation function to be used (a string, either â€œsigmoidâ€ or â€œreluâ€)
    Output:
    A â€“ the activations of the current layer
    linear_cache â€“ the dictionary generated by the linear_forward function
    """

    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation == "sigmoid":
        return sigmoid(Z), linear_cache
    elif activation == "relu":
        return relu(Z), linear_cache


def L_model_forward(X, parameters):
    """
    Description: Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    Input:
    X â€“ the data, numpy array of shape (input size, number of examples)
    parameters â€“ the initialized W and b parameters of each layer
    Output:
    AL â€“ the last post-activation value
    caches â€“ a list of all the cache objects generated by the linear_forward function
    """
    caches = []
    L = len(parameters) / 2 # parameters is a dictionary which holds Wl and bl for each layer l
    A = X
    for i in range(1,L):
        A, linear_cache = linear_activation_forward(A,parameters['W'+str(i)], parameters['b'+str(i)], "relu")
        caches.append(linear_cache)
    AL, linear_cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], "sigmoid")
    caches.append(linear_cache)

    return AL, caches

def compute_cost(AL, Y):
    """
    Description: Implement the cost function defined by equation
    ð‘ð‘œð‘ ð‘¡=âˆ’1ð‘šâˆ—Î£[(ð‘¦ð‘–âˆ—log(ð´ð¿))+((1âˆ’ð‘¦ð‘–)âˆ—(1âˆ’ð´ð¿))]ð‘š1 (see the slides of the first lecture for additional information if needed).
    Input:
    AL â€“ probability vector corresponding to your label predictions, shape (1, number of examples)
    Y â€“ the labels vector (i.e. the ground truth)
    Output:
    cost â€“ the cross-entropy cost
    """
    #TODO verify len(Y): Y is (m,1) or(1,m)?? Answer: Y is (1,m)
    cost = -1.0/Y.shape[1] * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))
    return cost



def linear_activation_backward(dA, cache, activation):
    """
    Description:
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.
    ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼ï¿¼
    Input:
    dA â€“ post activation gradient of the current layer
    cache â€“ contains both the linear cache and the activations cache
    Output:
    dA_prev â€“ Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW â€“ Gradient of the cost with respect to W (current layer l), same shape as W
    db â€“ Gradient of the cost with respect to b (current layer l), same shape as b

    """
    if activation == "sigmoid":
        dZ=sigmoid_backward(dA,cache)
    elif activation == "relu":
        dZ=relu_backward(dA,cache)
    else:
        raise NotImplemented("No Such activation function")
    return linear_backward(dZ,cache)


def linear_backward(dZ,cache):
    """
    Description :Implements the linear part of the backward propagation process for a single layer

    Input:
    dZ â€“ the gradient of the cost with respect to the linear output of the current layer (layer l)
    cache â€“ tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Output:
    dA_prev - Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW - Gradient of the cost with respect to W (current layer l), same shape as W
    db - Gradient of the cost with respect to b (current layer l), same shape as b
    """
    dA_prev=dZ+cache['A_prev']
    dW=dZ*cache['A_prev'].transpose()
    db=dZ
    return dA_prev,dW,db



def relu_backward (dA, activation_cache):
    """
    Description:
    Implements backward propagation for a ReLU unit
    Input:
    dA â€“ the post-activation gradient
    activation_cache â€“ contains Z (stored during the forward propagation)
    Output:
    dZ â€“ gradient of the cost with respect to Z
    """
    if activation_cache["Z"]<0:
        gz=0
    else:
        gz=1
    dZ=np.dot(dA,gz)
    return  dZ


def sigmoid_backward (dA, activation_cache):
    """
    Description:
    Implements backward propagation for a sigmoid unit
    Input:
    dA â€“ the post-activation gradient
    activation_cache â€“ contains Z (stored during the forward propagation)
    Output:
    dZ â€“ gradient of the cost with respect to Z
    """
    a,_=sigmoid(activation_cache["Z"])
    gz=a*(1-a)
    dZ = np.dot(dA, gz)
    return  dZ


def L_model_backward(AL, Y, caches):
    """
    Description:
    Implement the backward propagation process for the entire network.
    Some comments:
    - The backpropagation for the Sigmoid should be done separately (because there is only one like it), and the process for the ReLU layers should be done in a loop
    - The derivative for the output of the softmax layer can be calculated using: dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    Input:
    AL - the probabilities vector, the output of the forward propagation (L_model_forward)
    Y â€“ the true labels vector (the â€œground truthâ€ â€“ true classifications)
    Caches â€“ list of caches containing for each layer: a) the linear cache; b) the activation cache
    Output:
    Grads â€“ a dictionary with the gradients
    grads["dA" + str(l)] = ... grads["dW" + str(l)] = ... grads["db" + str(l)] = ...

    """
    dAL= -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    n_layers=len(caches.keys())
    dA_prev, dW, db=linear_activation_backward(dAL,caches[-1],'sigmoid')
    grads={"dA"+str(n_layers):dA_prev,"dW"+str(n_layers):dW,"db"+str(n_layers):db}
    for i in range (n_layers-1,0,-1):
        dA_prev, dW, db = linear_activation_backward(dA_prev, i, 'relu')
        grads.update({"dA"+str(i):dA_prev,"dW"+str(i):dW,"db"+str(i):db})
    return grads




def Update_parameters(parameters, grads, learning_rate):
    """
    Description:
    Updates parameters using gradient descent
    Input:
    parameters â€“ a python dictionary containing the DNN architectureâ€™s parameters
    grads â€“ a python dictionary containing the gradients (generated by L_model_backward)
    learning_rate â€“ the learning rate used to update the parameters (the â€œalphaâ€)
    Output:
    parameters â€“ the updated values of the parameters object provided as input
    """

if __name__ == '__main__':
    init = initialize_parameters([10,3,4,5,6,2,1])
    # A = np.expand_dims(np.array([1,1,1,1,1,1,1,1,1,1]), axis=1)
    A = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]).T
    print ('W1 shape:', init['W1'].shape)
    print (init['W1'])
    print('A shape:', A.shape)
    print (A)
    print('W*A:', np.dot(init['W1'], A), np.dot(init['W1'], A).shape)
    print('b1 shape:', init['b1'].shape)
    print(init['b1'])

    print (init['W1'])
    A_new, cash = linear_forward(A, init['W1'], init['b1'])
    print (A_new)

    print ("-------------------")
    print ('W2 shape:', init['W2'].shape)
    print (init['W2'])
    print('A_new shape:', A_new.shape)
    print (A_new)
    print('b2 shape:', init['b2'].shape)
    print(init['b2'])
    print(linear_forward(A_new, init['W2'], init['b2']))

    print (sigmoid(np.array([[-100,-100,-100],[100,100,100],[1,1,1]])))
    print (relu(np.array([[0.1,0.1,0.1],[-2,-2,-2],[1,1,1]])))

    (x_train, y_train), (x_test, y_test)  = load_data_set([1,2])
    print ((x_train.shape, y_train.shape), (x_test.shape, y_test.shape))


